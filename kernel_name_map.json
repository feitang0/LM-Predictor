{
  "torch.nn.functional.softmax": "F.softmax",
  "torch.nn.functional.linear": "F.linear",
  "torch.nn.functional.silu": "F.silu",
  "torch.nn.functional.gelu": "F.gelu",
  "torch.nn.functional.relu": "F.relu",
  "torch.nn.functional.dropout": "F.dropout",
  "torch.nn.functional.layer_norm": "F.layer_norm",
  "torch.nn.functional.embedding": "F.embedding",
  "torch.nn.functional.scaled_dot_product_attention": "F.scaled_dot_product_attention",
  "torch.nn.functional.batch_norm": "F.batch_norm",
  "torch.nn.functional.group_norm": "F.group_norm",
  "torch.nn.functional.instance_norm": "F.instance_norm",
  "torch.nn.functional.prelu": "F.prelu",
  "torch.nn.functional.elu": "F.elu",
  "torch.nn.functional.leaky_relu": "F.leaky_relu",
  "torch.nn.functional.relu6": "F.relu6",
  "torch.nn.Dropout": "F.dropout",
  "torch.nn.Linear": "F.linear",
  "torch.nn.Embedding": "F.embedding",
  "torch.nn.LayerNorm": "F.layer_norm",
  "torch.nn.BatchNorm1d": "F.batch_norm",
  "torch.nn.BatchNorm2d": "F.batch_norm",
  "torch.nn.GroupNorm": "F.group_norm",
  "torch.nn.SiLU": "F.silu",
  "torch.nn.GELU": "F.gelu",
  "torch.nn.ReLU": "F.relu",
  "torch.nn.PReLU": "F.prelu",
  "torch.nn.ELU": "F.elu",
  "torch.nn.LeakyReLU": "F.leaky_relu",
  "torch.nn.ReLU6": "F.relu6"
}
