{
  "class_name": "transformers.models.llama.modeling_llama.LlamaModel",
  "kernels": [
    {
      "kernel_type": "module_call",
      "operation": "Input embedding lookup",
      "analysis": "The embed_tokens module is a torch.nn.Embedding layer. Input_ids has shape ({batch_size}, {seq_len}), output has shape ({batch_size}, {seq_len}, {hidden_size}). This performs a lookup operation from the vocabulary to the hidden dimension.",
      "flops": "${torch.nn.modules.sparse.Embedding}({batch_size}, {seq_len}, {vocab_size}, {hidden_size})",
      "memory_access": {
        "read": "${torch.nn.modules.sparse.Embedding}({batch_size}, {seq_len}, {vocab_size}, {hidden_size})",
        "write": "${torch.nn.modules.sparse.Embedding}({batch_size}, {seq_len}, {vocab_size}, {hidden_size})"
      }
    },
    {
      "kernel_type": "direct_operation",
      "operation": "Cache position creation",
      "analysis": "Creates a 1D tensor of length {seq_len} using torch.arange. This is a metadata operation with minimal computation that defines the position indices for the current sequence.",
      "flops": "{seq_len}",
      "memory_access": {
        "read": "0",
        "write": "{seq_len} * {a_bytes}"
      }
    },
    {
      "kernel_type": "direct_operation",
      "operation": "Causal mask computation - full tensor creation",
      "analysis": "Creates a 2D tensor of shape ({seq_len}, {cache_len}) filled with minimum dtype values. This initializes the base causal mask structure.",
      "flops": "{seq_len} * {cache_len}",
      "memory_access": {
        "read": "0",
        "write": "{seq_len} * {cache_len} * {a_bytes}"
      }
    },
    {
      "kernel_type": "direct_operation",
      "operation": "Causal mask computation - triangular mask creation",
      "analysis": "Creates an upper triangular mask where elements above the diagonal are set to minimum dtype values. This implements the causal attention pattern where tokens can only attend to previous tokens.",
      "flops": "{seq_len} * {cache_len}",
      "memory_access": {
        "read": "{seq_len} * {cache_len} * {a_bytes}",
        "write": "{seq_len} * {cache_len} * {a_bytes}"
      }
    },
    {
      "kernel_type": "direct_operation",
      "operation": "Causal mask computation - position-based masking",
      "analysis": "Applies position-based masking by comparing cache positions and applying element-wise multiplication. Creates a boolean mask comparing positions and applies it to causal_mask to handle KV cache positions.",
      "flops": "2 * {seq_len} * {cache_len}",
      "memory_access": {
        "read": "2 * {seq_len} * {cache_len} * {a_bytes}",
        "write": "{seq_len} * {cache_len} * {a_bytes}"
      }
    },
    {
      "kernel_type": "direct_operation",
      "operation": "Causal mask computation - attention mask integration (2D case)",
      "analysis": "Combines causal mask with attention mask for padding handling. Involves element-wise addition, comparison, and masked fill operations to integrate padding information from attention_mask into the causal mask.",
      "flops": "3 * {batch_size} * {seq_len} * {mask_length}",
      "memory_access": {
        "read": "3 * {batch_size} * {seq_len} * {mask_length} * {a_bytes}",
        "write": "{batch_size} * {seq_len} * {mask_length} * {a_bytes}"
      }
    },
    {
      "kernel_type": "module_call",
      "operation": "Decoder layers processing",
      "analysis": "Iterative processing through all decoder layers. Each decoder_layer call contains significant computation including self-attention, MLP, and residual connections. The loop runs {num_layers} times.",
      "flops": "{num_layers} * ${transformers.models.llama.modeling_llama.LlamaDecoderLayer}({batch_size}, {seq_len}, {hidden_size}, {num_heads}, {head_dim}, {cache_len})",
      "memory_access": {
        "read": "{num_layers} * ${transformers.models.llama.modeling_llama.LlamaDecoderLayer}({batch_size}, {seq_len}, {hidden_size}, {num_heads}, {head_dim}, {cache_len})",
        "write": "{num_layers} * ${transformers.models.llama.modeling_llama.LlamaDecoderLayer}({batch_size}, {seq_len}, {hidden_size}, {num_heads}, {head_dim}, {cache_len})"
      }
    },
    {
      "kernel_type": "module_call",
      "operation": "Final RMS normalization",
      "analysis": "Applies RMS normalization to the final hidden states after all decoder layers. The norm module is a LlamaRMSNorm layer that normalizes across the hidden dimension.",
      "flops": "${transformers.models.llama.modeling_llama.LlamaRMSNorm}({batch_size}, {seq_len}, {hidden_size})",
      "memory_access": {
        "read": "${transformers.models.llama.modeling_llama.LlamaRMSNorm}({batch_size}, {seq_len}, {hidden_size})",
        "write": "${transformers.models.llama.modeling_llama.LlamaRMSNorm}({batch_size}, {seq_len}, {hidden_size})"
      }
    }
  ]
}