{
  "class_name": "transformers.models.llama.modeling_llama.LlamaDecoderLayer",
  "kernels": [
    {
      "kernel_type": "module_call",
      "operation": "Input layer normalization",
      "analysis": "The input_layernorm module is a LlamaRMSNorm layer applied to hidden_states with shape (batch_size, seq_len, hidden_size). This operation normalizes the input before self-attention computation.",
      "flops": "${transformers.models.llama.modeling_llama.LlamaRMSNorm}({batch_size}, {seq_len}, {hidden_size})",
      "memory_access": {
        "read": "${transformers.models.llama.modeling_llama.LlamaRMSNorm}({batch_size}, {seq_len}, {hidden_size})",
        "write": "${transformers.models.llama.modeling_llama.LlamaRMSNorm}({batch_size}, {seq_len}, {hidden_size})"
      }
    },
    {
      "kernel_type": "module_call",
      "operation": "Self-attention computation",
      "analysis": "The self_attn module performs multi-head self-attention including QKV projections, attention computation, and output projection. Input has shape (batch_size, seq_len, hidden_size), output has same shape. This includes attention computation with KV cache of length cache_len.",
      "flops": "${transformers.models.llama.modeling_llama.LlamaAttention}({batch_size}, {seq_len}, {hidden_size}, {num_heads}, {head_dim}, {cache_len})",
      "memory_access": {
        "read": "${transformers.models.llama.modeling_llama.LlamaAttention}({batch_size}, {seq_len}, {hidden_size}, {num_heads}, {head_dim}, {cache_len})",
        "write": "${transformers.models.llama.modeling_llama.LlamaAttention}({batch_size}, {seq_len}, {hidden_size}, {num_heads}, {head_dim}, {cache_len})"
      }
    },
    {
      "kernel_type": "direct_operation",
      "operation": "First residual connection addition",
      "analysis": "Element-wise addition between the residual (original input) and the self-attention output. Both tensors have shape (batch_size, seq_len, hidden_size), requiring batch_size * seq_len * hidden_size element-wise additions.",
      "flops": "{batch_size} * {seq_len} * {hidden_size}",
      "memory_access": {
        "read": "2 * {batch_size} * {seq_len} * {hidden_size} * {a_bytes}",
        "write": "{batch_size} * {seq_len} * {hidden_size} * {a_bytes}"
      }
    },
    {
      "kernel_type": "module_call",
      "operation": "Post-attention layer normalization",
      "analysis": "The post_attention_layernorm module is a LlamaRMSNorm layer applied to hidden_states after self-attention. Input has shape (batch_size, seq_len, hidden_size). This normalizes the output before MLP computation.",
      "flops": "${transformers.models.llama.modeling_llama.LlamaRMSNorm}({batch_size}, {seq_len}, {hidden_size})",
      "memory_access": {
        "read": "${transformers.models.llama.modeling_llama.LlamaRMSNorm}({batch_size}, {seq_len}, {hidden_size})",
        "write": "${transformers.models.llama.modeling_llama.LlamaRMSNorm}({batch_size}, {seq_len}, {hidden_size})"
      }
    },
    {
      "kernel_type": "module_call",
      "operation": "MLP computation",
      "analysis": "The mlp module performs feed-forward computation including gate projection, up projection, and down projection. Input has shape (batch_size, seq_len, hidden_size), output has same shape. This includes SiLU activation and projection to intermediate_size.",
      "flops": "${transformers.models.llama.modeling_llama.LlamaMLP}({batch_size}, {seq_len}, {hidden_size}, {intermediate_size})",
      "memory_access": {
        "read": "${transformers.models.llama.modeling_llama.LlamaMLP}({batch_size}, {seq_len}, {hidden_size}, {intermediate_size})",
        "write": "${transformers.models.llama.modeling_llama.LlamaMLP}({batch_size}, {seq_len}, {hidden_size}, {intermediate_size})"
      }
    },
    {
      "kernel_type": "direct_operation",
      "operation": "Second residual connection addition",
      "analysis": "Element-wise addition between the residual (post-attention output) and the MLP output. Both tensors have shape (batch_size, seq_len, hidden_size), requiring batch_size * seq_len * hidden_size element-wise additions.",
      "flops": "{batch_size} * {seq_len} * {hidden_size}",
      "memory_access": {
        "read": "2 * {batch_size} * {seq_len} * {hidden_size} * {a_bytes}",
        "write": "{batch_size} * {seq_len} * {hidden_size} * {a_bytes}"
      }
    }
  ]
}