{
  "class_name": "transformers.models.llama.modeling_llama.LlamaRMSNorm",
  "kernels": [
    {
      "kernel_type": "direct_operation",
      "operation": "Data type conversion to float32",
      "analysis": "Converts hidden_states from original dtype to float32. This is a memory operation, not computational. Input shape: (batch_size, seq_len, hidden_size). Output shape: (batch_size, seq_len, hidden_size).",
      "flops": "0",
      "memory_access": {
        "read": "{batch_size} * {seq_len} * {hidden_size} * {a_bytes}",
        "write": "{batch_size} * {seq_len} * {hidden_size} * 4"
      }
    },
    {
      "kernel_type": "direct_operation",
      "operation": "Variance computation (squaring and mean reduction)",
      "analysis": "Element-wise squaring of hidden_states followed by mean reduction along the last dimension (hidden_size). Input shape: (batch_size, seq_len, hidden_size). Output shape: (batch_size, seq_len, 1). Operations: hidden_states.pow(2) (squaring) and .mean(-1, keepdim=True) (mean reduction).",
      "flops": "2 * {batch_size} * {seq_len} * {hidden_size}",
      "memory_access": {
        "read": "{batch_size} * {seq_len} * {hidden_size} * 4",
        "write": "{batch_size} * {seq_len} * {hidden_size} * 4 + {batch_size} * {seq_len} * 4"
      }
    },
    {
      "kernel_type": "direct_operation",
      "operation": "Normalization with reciprocal square root",
      "analysis": "Element-wise multiplication with reciprocal square root of (variance + epsilon). Input shapes: hidden_states (batch_size, seq_len, hidden_size), variance (batch_size, seq_len, 1) broadcasted, epsilon (scalar). Operations: variance + epsilon (addition), torch.rsqrt() (reciprocal square root), hidden_states * result (element-wise multiplication).",
      "flops": "{batch_size} * {seq_len} * ({hidden_size} + 2)",
      "memory_access": {
        "read": "{batch_size} * {seq_len} * {hidden_size} * 4 + {batch_size} * {seq_len} * 4 + 4",
        "write": "{batch_size} * {seq_len} * {hidden_size} * 4"
      }
    },
    {
      "kernel_type": "direct_operation",
      "operation": "Weight scaling and data type conversion",
      "analysis": "Element-wise multiplication with weight parameter and conversion back to original data type. Input shapes: hidden_states (batch_size, seq_len, hidden_size), self.weight (hidden_size,) broadcasted. Operations: hidden_states.to(input_dtype) (data type conversion), self.weight * result (element-wise multiplication).",
      "flops": "{batch_size} * {seq_len} * {hidden_size}",
      "memory_access": {
        "read": "{batch_size} * {seq_len} * {hidden_size} * 4 + {hidden_size} * {w_bytes}",
        "write": "{batch_size} * {seq_len} * {hidden_size} * {a_bytes}"
      }
    }
  ]
}