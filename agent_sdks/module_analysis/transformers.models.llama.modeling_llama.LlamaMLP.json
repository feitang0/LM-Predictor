{
  "class_name": "transformers.models.llama.modeling_llama.LlamaMLP",
  "kernels": [
    {
      "kernel_type": "module_call",
      "operation": "Gate projection linear layer",
      "analysis": "Linear projection from hidden_size to intermediate_size. Input tensor x has shape ({batch_size}, {seq_len}, {hidden_size}), output has shape ({batch_size}, {seq_len}, {intermediate_size}). This is a torch.nn.Linear module that performs matrix multiplication.",
      "flops": "${torch.nn.modules.linear.Linear}({batch_size}, {seq_len}, {hidden_size}, {intermediate_size})",
      "memory_access": {
        "read": "${torch.nn.modules.linear.Linear}({batch_size}, {seq_len}, {hidden_size}, {intermediate_size})",
        "write": "${torch.nn.modules.linear.Linear}({batch_size}, {seq_len}, {hidden_size}, {intermediate_size})"
      }
    },
    {
      "kernel_type": "module_call",
      "operation": "SiLU activation function",
      "analysis": "SiLU (Swish) activation function applied element-wise to gate projection output. Input and output tensors have shape ({batch_size}, {seq_len}, {intermediate_size}). SiLU requires computing sigmoid and element-wise multiplication.",
      "flops": "${torch.nn.modules.activation.SiLU}({batch_size}, {seq_len}, {intermediate_size})",
      "memory_access": {
        "read": "${torch.nn.modules.activation.SiLU}({batch_size}, {seq_len}, {intermediate_size})",
        "write": "${torch.nn.modules.activation.SiLU}({batch_size}, {seq_len}, {intermediate_size})"
      }
    },
    {
      "kernel_type": "module_call",
      "operation": "Up projection linear layer",
      "analysis": "Linear projection from hidden_size to intermediate_size. Input tensor x has shape ({batch_size}, {seq_len}, {hidden_size}), output has shape ({batch_size}, {seq_len}, {intermediate_size}). This is a torch.nn.Linear module that performs matrix multiplication.",
      "flops": "${torch.nn.modules.linear.Linear}({batch_size}, {seq_len}, {hidden_size}, {intermediate_size})",
      "memory_access": {
        "read": "${torch.nn.modules.linear.Linear}({batch_size}, {seq_len}, {hidden_size}, {intermediate_size})",
        "write": "${torch.nn.modules.linear.Linear}({batch_size}, {seq_len}, {hidden_size}, {intermediate_size})"
      }
    },
    {
      "kernel_type": "direct_operation",
      "operation": "Element-wise multiplication between activated gate and up projection",
      "analysis": "Element-wise multiplication between the activated gate projection (SiLU output) and up projection. Both operands have shape ({batch_size}, {seq_len}, {intermediate_size}), output has same shape. This is a direct tensor operation requiring one multiplication per element.",
      "flops": "{batch_size} * {seq_len} * {intermediate_size}",
      "memory_access": {
        "read": "2 * {batch_size} * {seq_len} * {intermediate_size} * {a_bytes}",
        "write": "{batch_size} * {seq_len} * {intermediate_size} * {a_bytes}"
      }
    },
    {
      "kernel_type": "module_call",
      "operation": "Down projection linear layer",
      "analysis": "Linear projection from intermediate_size back to hidden_size. Input has shape ({batch_size}, {seq_len}, {intermediate_size}), output has shape ({batch_size}, {seq_len}, {hidden_size}). This is a torch.nn.Linear module that performs matrix multiplication.",
      "flops": "${torch.nn.modules.linear.Linear}({batch_size}, {seq_len}, {intermediate_size}, {hidden_size})",
      "memory_access": {
        "read": "${torch.nn.modules.linear.Linear}({batch_size}, {seq_len}, {intermediate_size}, {hidden_size})",
        "write": "${torch.nn.modules.linear.Linear}({batch_size}, {seq_len}, {intermediate_size}, {hidden_size})"
      }
    }
  ]
}