{
  "class_name": "transformers.models.llama.modeling_llama.LlamaSdpaAttention",
  "kernels": [
    {
      "kernel_type": "module_call",
      "operation": "Query projection through linear layer",
      "analysis": "The q_proj module is a Linear layer. Input shape: ({batch_size}, {seq_len}, {hidden_size}), output shape: ({batch_size}, {seq_len}, {num_heads} * {head_dim}). This performs a matrix multiplication from hidden_size to num_heads * head_dim dimensions.",
      "flops": "${torch.nn.modules.linear.Linear}({batch_size}, {seq_len}, {hidden_size}, {num_heads}, {head_dim})",
      "memory_access": {
        "read": "${torch.nn.modules.linear.Linear}({batch_size}, {seq_len}, {hidden_size}, {num_heads}, {head_dim})",
        "write": "${torch.nn.modules.linear.Linear}({batch_size}, {seq_len}, {hidden_size}, {num_heads}, {head_dim})"
      }
    },
    {
      "kernel_type": "module_call",
      "operation": "Key projection through linear layer",
      "analysis": "The k_proj module is a Linear layer. Input shape: ({batch_size}, {seq_len}, {hidden_size}), output shape: ({batch_size}, {seq_len}, {num_key_value_heads} * {head_dim}). This performs a matrix multiplication from hidden_size to num_key_value_heads * head_dim dimensions.",
      "flops": "${torch.nn.modules.linear.Linear}({batch_size}, {seq_len}, {hidden_size}, {num_key_value_heads}, {head_dim})",
      "memory_access": {
        "read": "${torch.nn.modules.linear.Linear}({batch_size}, {seq_len}, {hidden_size}, {num_key_value_heads}, {head_dim})",
        "write": "${torch.nn.modules.linear.Linear}({batch_size}, {seq_len}, {hidden_size}, {num_key_value_heads}, {head_dim})"
      }
    },
    {
      "kernel_type": "module_call",
      "operation": "Value projection through linear layer",
      "analysis": "The v_proj module is a Linear layer. Input shape: ({batch_size}, {seq_len}, {hidden_size}), output shape: ({batch_size}, {seq_len}, {num_key_value_heads} * {head_dim}). This performs a matrix multiplication from hidden_size to num_key_value_heads * head_dim dimensions.",
      "flops": "${torch.nn.modules.linear.Linear}({batch_size}, {seq_len}, {hidden_size}, {num_key_value_heads}, {head_dim})",
      "memory_access": {
        "read": "${torch.nn.modules.linear.Linear}({batch_size}, {seq_len}, {hidden_size}, {num_key_value_heads}, {head_dim})",
        "write": "${torch.nn.modules.linear.Linear}({batch_size}, {seq_len}, {hidden_size}, {num_key_value_heads}, {head_dim})"
      }
    },
    {
      "kernel_type": "module_call",
      "operation": "Rotary positional embedding computation",
      "analysis": "The rotary_emb module computes cosine and sine values for rotary positional embeddings. Input shape: ({batch_size}, {num_key_value_heads}, {seq_len}, {head_dim}). This computes trigonometric functions for positional encoding.",
      "flops": "${transformers.models.llama.modeling_llama.LlamaRotaryEmbedding}({batch_size}, {num_key_value_heads}, {seq_len}, {head_dim})",
      "memory_access": {
        "read": "${transformers.models.llama.modeling_llama.LlamaRotaryEmbedding}({batch_size}, {num_key_value_heads}, {seq_len}, {head_dim})",
        "write": "${transformers.models.llama.modeling_llama.LlamaRotaryEmbedding}({batch_size}, {num_key_value_heads}, {seq_len}, {head_dim})"
      }
    },
    {
      "kernel_type": "direct_operation",
      "operation": "Apply rotary positional embeddings",
      "analysis": "This function applies the rotary embeddings to query and key states. It performs element-wise operations to rotate the vectors. Query states shape: ({batch_size}, {num_heads}, {seq_len}, {head_dim}), key states shape: ({batch_size}, {num_key_value_heads}, {seq_len}, {head_dim}). Each element requires trigonometric operations for rotation.",
      "flops": "2 * {batch_size} * {num_heads} * {seq_len} * {head_dim} + 2 * {batch_size} * {num_key_value_heads} * {seq_len} * {head_dim}",
      "memory_access": {
        "read": "({batch_size} * {num_heads} * {seq_len} * {head_dim} + {batch_size} * {num_key_value_heads} * {seq_len} * {head_dim} + 2 * {batch_size} * {num_key_value_heads} * {seq_len} * {head_dim}) * {a_bytes}",
        "write": "({batch_size} * {num_heads} * {seq_len} * {head_dim} + {batch_size} * {num_key_value_heads} * {seq_len} * {head_dim}) * {a_bytes}"
      }
    },
    {
      "kernel_type": "direct_operation",
      "operation": "Update key-value cache",
      "analysis": "The past_key_value.update() method updates the KV cache. This involves concatenating new key/value states with cached ones. For inference with use_cache=True, this updates the cache. Input shapes: key_states ({batch_size}, {num_key_value_heads}, {seq_len}, {head_dim}), value_states ({batch_size}, {num_key_value_heads}, {seq_len}, {head_dim}).",
      "flops": "0",
      "memory_access": {
        "read": "2 * {batch_size} * {num_key_value_heads} * {seq_len} * {head_dim} * {a_bytes}",
        "write": "2 * {batch_size} * {num_key_value_heads} * ({cache_len} + {seq_len}) * {head_dim} * {a_bytes}"
      }
    },
    {
      "kernel_type": "direct_operation",
      "operation": "Repeat key states for grouped query attention",
      "analysis": "Repeats key states to match the number of query heads. Input shape: ({batch_size}, {num_key_value_heads}, {seq_len}, {head_dim}), output shape: ({batch_size}, {num_heads}, {seq_len}, {head_dim}). This is a memory operation with no computation.",
      "flops": "0",
      "memory_access": {
        "read": "{batch_size} * {num_key_value_heads} * {seq_len} * {head_dim} * {a_bytes}",
        "write": "{batch_size} * {num_heads} * {seq_len} * {head_dim} * {a_bytes}"
      }
    },
    {
      "kernel_type": "direct_operation",
      "operation": "Repeat value states for grouped query attention",
      "analysis": "Repeats value states to match the number of query heads. Input shape: ({batch_size}, {num_key_value_heads}, {seq_len}, {head_dim}), output shape: ({batch_size}, {num_heads}, {seq_len}, {head_dim}). This is a memory operation with no computation.",
      "flops": "0",
      "memory_access": {
        "read": "{batch_size} * {num_key_value_heads} * {seq_len} * {head_dim} * {a_bytes}",
        "write": "{batch_size} * {num_heads} * {seq_len} * {head_dim} * {a_bytes}"
      }
    },
    {
      "kernel_type": "module_call",
      "operation": "Scaled dot product attention computation",
      "analysis": "This is the core attention computation using torch.nn.functional.scaled_dot_product_attention. For inference with training=False, dropout_p=0.0. Input shapes: query_states ({batch_size}, {num_heads}, {seq_len}, {head_dim}), key_states ({batch_size}, {num_heads}, {cache_len}, {head_dim}), value_states ({batch_size}, {num_heads}, {cache_len}, {head_dim}).",
      "flops": "${torch.nn.functional.scaled_dot_product_attention}({batch_size}, {num_heads}, {seq_len}, {head_dim}, {cache_len})",
      "memory_access": {
        "read": "${torch.nn.functional.scaled_dot_product_attention}({batch_size}, {num_heads}, {seq_len}, {head_dim}, {cache_len})",
        "write": "${torch.nn.functional.scaled_dot_product_attention}({batch_size}, {num_heads}, {seq_len}, {head_dim}, {cache_len})"
      }
    },
    {
      "kernel_type": "module_call",
      "operation": "Output projection through linear layer",
      "analysis": "The o_proj module is a Linear layer. Input shape: ({batch_size}, {seq_len}, {hidden_size}), output shape: ({batch_size}, {seq_len}, {hidden_size}). This performs a matrix multiplication from hidden_size to hidden_size dimensions.",
      "flops": "${torch.nn.modules.linear.Linear}({batch_size}, {seq_len}, {hidden_size}, {hidden_size})",
      "memory_access": {
        "read": "${torch.nn.modules.linear.Linear}({batch_size}, {seq_len}, {hidden_size}, {hidden_size})",
        "write": "${torch.nn.modules.linear.Linear}({batch_size}, {seq_len}, {hidden_size}, {hidden_size})"
      }
    }
  ]
}