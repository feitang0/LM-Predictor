{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Model Architecture Representation Schema",
  "description": "JSON Schema for standardized model architecture representation using hierarchical layer structure",
  "type": "object",
  "properties": {
    "model_id": {
      "type": "string",
      "description": "Model identifier (e.g., HuggingFace model ID or custom name)",
      "examples": ["meta-llama/Llama-2-7b-hf", "gpt-3.5-turbo", "bert-base-uncased"]
    },
    "layers": {
      "type": "array",
      "description": "Top-level hierarchical layer structure of the model",
      "items": {"$ref": "#/definitions/layer"},
      "minItems": 1
    }
  },
  "required": ["model_id", "layers"],
  "additionalProperties": false,
  "definitions": {
    "layer": {
      "type": "object",
      "description": "A single layer in the model hierarchy. Can be basic (computational) or composite (organizational)",
      "properties": {
        "name": {
          "type": "string",
          "description": "Layer name as it appears in the model",
          "examples": ["embed_tokens", "decoder_layer", "self_attn", "q_proj", "lm_head"]
        },
        "class": {
          "type": "string",
          "description": "Full Python class path for the layer",
          "pattern": "^[a-zA-Z_][a-zA-Z0-9_.]*\\.[A-Z][a-zA-Z0-9_]*$",
          "examples": [
            "torch.nn.modules.linear.Linear",
            "torch.nn.modules.sparse.Embedding",
            "transformers.models.llama.modeling_llama.LlamaDecoderLayer",
            "transformers.models.llama.modeling_llama.LlamaSdpaAttention"
          ]
        },
        "repeat": {
          "type": "integer",
          "description": "Number of times this layer structure is repeated (optional, defaults to 1)",
          "minimum": 1,
          "examples": [32, 12, 24]
        },
        "sub_layers": {
          "type": "array",
          "description": "Nested layers within this layer (makes this a composite layer)",
          "items": {"$ref": "#/definitions/layer"},
          "minItems": 1
        }
      },
      "required": ["name", "class"],
      "additionalProperties": false,
      "examples": [
        {
          "name": "embed_tokens",
          "class": "torch.nn.modules.sparse.Embedding"
        },
        {
          "name": "decoder_layer",
          "class": "transformers.models.llama.modeling_llama.LlamaDecoderLayer",
          "repeat": 32,
          "sub_layers": [
            {
              "name": "self_attn",
              "class": "transformers.models.llama.modeling_llama.LlamaSdpaAttention",
              "sub_layers": [
                {
                  "name": "q_proj",
                  "class": "torch.nn.modules.linear.Linear"
                }
              ]
            }
          ]
        }
      ]
    }
  }
}