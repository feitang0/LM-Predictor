{
  "model_id": "meta-llama/Llama-2-7b-hf",
  "layers": [
    {
      "name": "model",
      "class": "transformers.models.llama.modeling_llama.LlamaModel",
      "sub_layers": [
        {
          "name": "embed_tokens",
          "class": "torch.nn.modules.sparse.Embedding",
          "parameters": {
            "num_indices": "{batch_size} * {seq_len}",
            "embedding_dim": 4096,
            "dtype_bytes": "{dtype_bytes}",
            "index_dtype_bytes": "{index_dtype_bytes}"
          }
        },
        {
          "name": "decoder_layer",
          "class": "transformers.models.llama.modeling_llama.LlamaDecoderLayer",
          "repeat": 32,
          "sub_layers": [
            {
              "name": "self_attn",
              "class": "transformers.models.llama.modeling_llama.LlamaSdpaAttention",
              "sub_layers": [
                {
                  "name": "q_proj",
                  "class": "torch.nn.modules.linear.Linear",
                  "parameters": {
                    "N": "{batch_size} * {seq_len}",
                    "in_features": 4096,
                    "out_features": 4096,
                    "dtype_bytes": "{dtype_bytes}"
                  }
                },
                {
                  "name": "k_proj",
                  "class": "torch.nn.modules.linear.Linear",
                  "parameters": {
                    "N": "{batch_size} * {seq_len}",
                    "in_features": 4096,
                    "out_features": 4096,
                    "dtype_bytes": "{dtype_bytes}"
                  }
                },
                {
                  "name": "v_proj",
                  "class": "torch.nn.modules.linear.Linear",
                  "parameters": {
                    "N": "{batch_size} * {seq_len}",
                    "in_features": 4096,
                    "out_features": 4096,
                    "dtype_bytes": "{dtype_bytes}"
                  }
                },
                {
                  "name": "o_proj",
                  "class": "torch.nn.modules.linear.Linear",
                  "parameters": {
                    "N": "{batch_size} * {seq_len}",
                    "in_features": 4096,
                    "out_features": 4096,
                    "dtype_bytes": "{dtype_bytes}"
                  }
                },
                {
                  "name": "rotary_emb",
                  "class": "transformers.models.llama.modeling_llama.LlamaRotaryEmbedding",
                  "parameters": {
                    "B": "{batch_size}",
                    "S": "{seq_len}",
                    "head_dim": 128,
                    "dtype_bytes": "{dtype_bytes}"
                  }
                }
              ]
            },
            {
              "name": "mlp",
              "class": "transformers.models.llama.modeling_llama.LlamaMLP",
              "sub_layers": [
                {
                  "name": "gate_proj",
                  "class": "torch.nn.modules.linear.Linear",
                  "parameters": {
                    "N": "{batch_size} * {seq_len}",
                    "in_features": 4096,
                    "out_features": 11008,
                    "dtype_bytes": "{dtype_bytes}"
                  }
                },
                {
                  "name": "up_proj",
                  "class": "torch.nn.modules.linear.Linear",
                  "parameters": {
                    "N": "{batch_size} * {seq_len}",
                    "in_features": 4096,
                    "out_features": 11008,
                    "dtype_bytes": "{dtype_bytes}"
                  }
                },
                {
                  "name": "down_proj",
                  "class": "torch.nn.modules.linear.Linear",
                  "parameters": {
                    "N": "{batch_size} * {seq_len}",
                    "in_features": 11008,
                    "out_features": 4096,
                    "dtype_bytes": "{dtype_bytes}"
                  }
                },
                {
                  "name": "act_fn",
                  "class": "torch.nn.modules.activation.SiLU",
                  "parameters": {
                    "num_elements": "{batch_size} * {seq_len} * 11008",
                    "dtype_bytes": "{dtype_bytes}"
                  }
                }
              ]
            },
            {
              "name": "input_layernorm",
              "class": "transformers.models.llama.modeling_llama.LlamaRMSNorm",
              "parameters": {
                "B": "{batch_size}",
                "S": "{seq_len}",
                "hidden_size": 4096,
                "dtype_bytes": "{dtype_bytes}"
              }
            },
            {
              "name": "post_attention_layernorm",
              "class": "transformers.models.llama.modeling_llama.LlamaRMSNorm",
              "parameters": {
                "B": "{batch_size}",
                "S": "{seq_len}",
                "hidden_size": 4096,
                "dtype_bytes": "{dtype_bytes}"
              }
            }
          ]
        },
        {
          "name": "norm",
          "class": "transformers.models.llama.modeling_llama.LlamaRMSNorm",
          "parameters": {
            "B": "{batch_size}",
            "S": "{seq_len}",
            "hidden_size": 4096,
            "dtype_bytes": "{dtype_bytes}"
          }
        },
        {
          "name": "rotary_emb",
          "class": "transformers.models.llama.modeling_llama.LlamaRotaryEmbedding",
          "parameters": {
            "B": "{batch_size}",
            "S": "{seq_len}",
            "head_dim": 128,
            "dtype_bytes": "{dtype_bytes}"
          }
        }
      ]
    },
    {
      "name": "lm_head",
      "class": "torch.nn.modules.linear.Linear",
      "parameters": {
        "N": "{batch_size} * {seq_len}",
        "in_features": 4096,
        "out_features": 32000,
        "dtype_bytes": "{dtype_bytes}"
      }
    }
  ]
}