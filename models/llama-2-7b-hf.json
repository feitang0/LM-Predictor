{
  "model_id": "meta-llama/Llama-2-7b-hf",
  "layers": [
    {
      "name": "model",
      "class": "transformers.models.llama.modeling_llama.LlamaModel",
      "sub_layers": [
        {
          "name": "embed_tokens",
          "class": "torch.nn.modules.sparse.Embedding"
        },
        {
          "name": "decoder_layer",
          "class": "transformers.models.llama.modeling_llama.LlamaDecoderLayer",
          "repeat": 32,
          "sub_layers": [
            {
              "name": "self_attn",
              "class": "transformers.models.llama.modeling_llama.LlamaSdpaAttention",
              "sub_layers": [
                {
                  "name": "q_proj",
                  "class": "torch.nn.modules.linear.Linear"
                },
                {
                  "name": "k_proj",
                  "class": "torch.nn.modules.linear.Linear"
                },
                {
                  "name": "v_proj",
                  "class": "torch.nn.modules.linear.Linear"
                },
                {
                  "name": "o_proj",
                  "class": "torch.nn.modules.linear.Linear"
                },
                {
                  "name": "rotary_emb",
                  "class": "transformers.models.llama.modeling_llama.LlamaRotaryEmbedding"
                }
              ]
            },
            {
              "name": "mlp",
              "class": "transformers.models.llama.modeling_llama.LlamaMLP",
              "sub_layers": [
                {
                  "name": "gate_proj",
                  "class": "torch.nn.modules.linear.Linear"
                },
                {
                  "name": "up_proj",
                  "class": "torch.nn.modules.linear.Linear"
                },
                {
                  "name": "down_proj",
                  "class": "torch.nn.modules.linear.Linear"
                },
                {
                  "name": "act_fn",
                  "class": "torch.nn.modules.activation.SiLU"
                }
              ]
            },
            {
              "name": "input_layernorm",
              "class": "transformers.models.llama.modeling_llama.LlamaRMSNorm"
            },
            {
              "name": "post_attention_layernorm",
              "class": "transformers.models.llama.modeling_llama.LlamaRMSNorm"
            }
          ]
        },
        {
          "name": "norm",
          "class": "transformers.models.llama.modeling_llama.LlamaRMSNorm"
        },
        {
          "name": "rotary_emb",
          "class": "transformers.models.llama.modeling_llama.LlamaRotaryEmbedding"
        }
      ]
    },
    {
      "name": "lm_head",
      "class": "torch.nn.modules.linear.Linear"
    }
  ]
}