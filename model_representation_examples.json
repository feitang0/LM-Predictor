{
  "$schema": "./model_representation_schema.json",
  "title": "Model Architecture Representation Examples",
  "description": "Example model architectures following the standardized representation format",
  "examples": {
    "llama-2-7b-hf": {
      "description": "Complete Llama-2-7b-hf architecture showing hierarchical structure with basic and composite layers",
      "model_id": "meta-llama/Llama-2-7b-hf",
      "layers": [
        {
          "name": "model",
          "class": "transformers.models.llama.modeling_llama.LlamaModel",
          "sub_layers": [
            {
              "name": "embed_tokens",
              "class": "torch.nn.modules.sparse.Embedding"
            },
            {
              "name": "decoder_layer",
              "class": "transformers.models.llama.modeling_llama.LlamaDecoderLayer",
              "repeat": 32,
              "sub_layers": [
                {
                  "name": "self_attn",
                  "class": "transformers.models.llama.modeling_llama.LlamaSdpaAttention",
                  "sub_layers": [
                    {
                      "name": "q_proj",
                      "class": "torch.nn.modules.linear.Linear"
                    },
                    {
                      "name": "k_proj",
                      "class": "torch.nn.modules.linear.Linear"
                    },
                    {
                      "name": "v_proj",
                      "class": "torch.nn.modules.linear.Linear"
                    },
                    {
                      "name": "o_proj",
                      "class": "torch.nn.modules.linear.Linear"
                    },
                    {
                      "name": "rotary_emb",
                      "class": "transformers.models.llama.modeling_llama.LlamaRotaryEmbedding"
                    }
                  ]
                },
                {
                  "name": "mlp",
                  "class": "transformers.models.llama.modeling_llama.LlamaMLP",
                  "sub_layers": [
                    {
                      "name": "gate_proj",
                      "class": "torch.nn.modules.linear.Linear"
                    },
                    {
                      "name": "up_proj",
                      "class": "torch.nn.modules.linear.Linear"
                    },
                    {
                      "name": "down_proj",
                      "class": "torch.nn.modules.linear.Linear"
                    },
                    {
                      "name": "act_fn",
                      "class": "torch.nn.modules.activation.SiLU"
                    }
                  ]
                },
                {
                  "name": "input_layernorm",
                  "class": "transformers.models.llama.modeling_llama.LlamaRMSNorm"
                },
                {
                  "name": "post_attention_layernorm",
                  "class": "transformers.models.llama.modeling_llama.LlamaRMSNorm"
                }
              ]
            },
            {
              "name": "norm",
              "class": "transformers.models.llama.modeling_llama.LlamaRMSNorm"
            },
            {
              "name": "rotary_emb",
              "class": "transformers.models.llama.modeling_llama.LlamaRotaryEmbedding"
            }
          ]
        },
        {
          "name": "lm_head",
          "class": "torch.nn.modules.linear.Linear"
        }
      ]
    },
    "simple-classifier": {
      "description": "Simple feed-forward classifier showing basic architecture without repetition",
      "model_id": "simple-classifier",
      "layers": [
        {
          "name": "embedding",
          "class": "torch.nn.modules.sparse.Embedding"
        },
        {
          "name": "linear1",
          "class": "torch.nn.modules.linear.Linear"
        },
        {
          "name": "activation",
          "class": "torch.nn.modules.activation.ReLU"
        },
        {
          "name": "linear2",
          "class": "torch.nn.modules.linear.Linear"
        },
        {
          "name": "softmax",
          "class": "torch.nn.modules.activation.Softmax"
        }
      ]
    },
    "transformer-block": {
      "description": "Generic transformer with repeated encoder blocks",
      "model_id": "generic-transformer",
      "layers": [
        {
          "name": "embedding",
          "class": "torch.nn.modules.sparse.Embedding"
        },
        {
          "name": "encoder_block",
          "class": "torch.nn.modules.transformer.TransformerEncoderLayer",
          "repeat": 6,
          "sub_layers": [
            {
              "name": "self_attn",
              "class": "torch.nn.modules.activation.MultiheadAttention"
            },
            {
              "name": "linear1",
              "class": "torch.nn.modules.linear.Linear"
            },
            {
              "name": "dropout",
              "class": "torch.nn.modules.dropout.Dropout"
            },
            {
              "name": "linear2",
              "class": "torch.nn.modules.linear.Linear"
            },
            {
              "name": "norm1",
              "class": "torch.nn.modules.normalization.LayerNorm"
            },
            {
              "name": "norm2",
              "class": "torch.nn.modules.normalization.LayerNorm"
            }
          ]
        },
        {
          "name": "classifier_head",
          "class": "torch.nn.modules.linear.Linear"
        }
      ]
    }
  },
  "layer_classification": {
    "basic_layers": {
      "description": "Layers WITHOUT sub_layers field - perform actual computation",
      "examples": [
        "torch.nn.modules.linear.Linear",
        "torch.nn.modules.sparse.Embedding",
        "torch.nn.modules.activation.SiLU",
        "transformers.models.llama.modeling_llama.LlamaRMSNorm",
        "transformers.models.llama.modeling_llama.LlamaRotaryEmbedding"
      ]
    },
    "composite_layers": {
      "description": "Layers WITH sub_layers field - organizational containers",
      "examples": [
        "transformers.models.llama.modeling_llama.LlamaModel",
        "transformers.models.llama.modeling_llama.LlamaDecoderLayer",
        "transformers.models.llama.modeling_llama.LlamaSdpaAttention",
        "transformers.models.llama.modeling_llama.LlamaMLP"
      ]
    }
  }
}