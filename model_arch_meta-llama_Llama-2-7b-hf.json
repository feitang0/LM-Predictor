{
  "class": "LlamaForCausalLM",
  "children": {
    "model": {
      "class": "LlamaModel",
      "children": {
        "embed_tokens": {
          "class": "Embedding",
          "module": "Embedding(32000, 4096)"
        },
        "layers": {
          "class": "ModuleList",
          "children": {
            "0": {
              "class": "LlamaDecoderLayer",
              "children": {
                "self_attn": {
                  "class": "LlamaAttention",
                  "children": {
                    "q_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "k_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "v_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "o_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    }
                  }
                },
                "mlp": {
                  "class": "LlamaMLP",
                  "children": {
                    "gate_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "up_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "down_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=11008, out_features=4096, bias=False)"
                    },
                    "act_fn": {
                      "class": "SiLU",
                      "module": "SiLU()"
                    }
                  }
                },
                "input_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                },
                "post_attention_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                }
              }
            },
            "1": {
              "class": "LlamaDecoderLayer",
              "children": {
                "self_attn": {
                  "class": "LlamaAttention",
                  "children": {
                    "q_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "k_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "v_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "o_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    }
                  }
                },
                "mlp": {
                  "class": "LlamaMLP",
                  "children": {
                    "gate_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "up_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "down_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=11008, out_features=4096, bias=False)"
                    },
                    "act_fn": {
                      "class": "SiLU",
                      "module": "SiLU()"
                    }
                  }
                },
                "input_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                },
                "post_attention_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                }
              }
            },
            "2": {
              "class": "LlamaDecoderLayer",
              "children": {
                "self_attn": {
                  "class": "LlamaAttention",
                  "children": {
                    "q_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "k_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "v_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "o_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    }
                  }
                },
                "mlp": {
                  "class": "LlamaMLP",
                  "children": {
                    "gate_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "up_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "down_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=11008, out_features=4096, bias=False)"
                    },
                    "act_fn": {
                      "class": "SiLU",
                      "module": "SiLU()"
                    }
                  }
                },
                "input_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                },
                "post_attention_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                }
              }
            },
            "3": {
              "class": "LlamaDecoderLayer",
              "children": {
                "self_attn": {
                  "class": "LlamaAttention",
                  "children": {
                    "q_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "k_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "v_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "o_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    }
                  }
                },
                "mlp": {
                  "class": "LlamaMLP",
                  "children": {
                    "gate_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "up_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "down_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=11008, out_features=4096, bias=False)"
                    },
                    "act_fn": {
                      "class": "SiLU",
                      "module": "SiLU()"
                    }
                  }
                },
                "input_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                },
                "post_attention_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                }
              }
            },
            "4": {
              "class": "LlamaDecoderLayer",
              "children": {
                "self_attn": {
                  "class": "LlamaAttention",
                  "children": {
                    "q_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "k_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "v_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "o_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    }
                  }
                },
                "mlp": {
                  "class": "LlamaMLP",
                  "children": {
                    "gate_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "up_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "down_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=11008, out_features=4096, bias=False)"
                    },
                    "act_fn": {
                      "class": "SiLU",
                      "module": "SiLU()"
                    }
                  }
                },
                "input_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                },
                "post_attention_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                }
              }
            },
            "5": {
              "class": "LlamaDecoderLayer",
              "children": {
                "self_attn": {
                  "class": "LlamaAttention",
                  "children": {
                    "q_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "k_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "v_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "o_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    }
                  }
                },
                "mlp": {
                  "class": "LlamaMLP",
                  "children": {
                    "gate_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "up_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "down_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=11008, out_features=4096, bias=False)"
                    },
                    "act_fn": {
                      "class": "SiLU",
                      "module": "SiLU()"
                    }
                  }
                },
                "input_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                },
                "post_attention_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                }
              }
            },
            "6": {
              "class": "LlamaDecoderLayer",
              "children": {
                "self_attn": {
                  "class": "LlamaAttention",
                  "children": {
                    "q_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "k_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "v_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "o_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    }
                  }
                },
                "mlp": {
                  "class": "LlamaMLP",
                  "children": {
                    "gate_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "up_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "down_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=11008, out_features=4096, bias=False)"
                    },
                    "act_fn": {
                      "class": "SiLU",
                      "module": "SiLU()"
                    }
                  }
                },
                "input_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                },
                "post_attention_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                }
              }
            },
            "7": {
              "class": "LlamaDecoderLayer",
              "children": {
                "self_attn": {
                  "class": "LlamaAttention",
                  "children": {
                    "q_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "k_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "v_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "o_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    }
                  }
                },
                "mlp": {
                  "class": "LlamaMLP",
                  "children": {
                    "gate_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "up_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "down_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=11008, out_features=4096, bias=False)"
                    },
                    "act_fn": {
                      "class": "SiLU",
                      "module": "SiLU()"
                    }
                  }
                },
                "input_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                },
                "post_attention_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                }
              }
            },
            "8": {
              "class": "LlamaDecoderLayer",
              "children": {
                "self_attn": {
                  "class": "LlamaAttention",
                  "children": {
                    "q_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "k_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "v_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "o_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    }
                  }
                },
                "mlp": {
                  "class": "LlamaMLP",
                  "children": {
                    "gate_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "up_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "down_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=11008, out_features=4096, bias=False)"
                    },
                    "act_fn": {
                      "class": "SiLU",
                      "module": "SiLU()"
                    }
                  }
                },
                "input_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                },
                "post_attention_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                }
              }
            },
            "9": {
              "class": "LlamaDecoderLayer",
              "children": {
                "self_attn": {
                  "class": "LlamaAttention",
                  "children": {
                    "q_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "k_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "v_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "o_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    }
                  }
                },
                "mlp": {
                  "class": "LlamaMLP",
                  "children": {
                    "gate_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "up_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "down_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=11008, out_features=4096, bias=False)"
                    },
                    "act_fn": {
                      "class": "SiLU",
                      "module": "SiLU()"
                    }
                  }
                },
                "input_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                },
                "post_attention_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                }
              }
            },
            "10": {
              "class": "LlamaDecoderLayer",
              "children": {
                "self_attn": {
                  "class": "LlamaAttention",
                  "children": {
                    "q_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "k_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "v_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "o_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    }
                  }
                },
                "mlp": {
                  "class": "LlamaMLP",
                  "children": {
                    "gate_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "up_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "down_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=11008, out_features=4096, bias=False)"
                    },
                    "act_fn": {
                      "class": "SiLU",
                      "module": "SiLU()"
                    }
                  }
                },
                "input_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                },
                "post_attention_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                }
              }
            },
            "11": {
              "class": "LlamaDecoderLayer",
              "children": {
                "self_attn": {
                  "class": "LlamaAttention",
                  "children": {
                    "q_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "k_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "v_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "o_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    }
                  }
                },
                "mlp": {
                  "class": "LlamaMLP",
                  "children": {
                    "gate_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "up_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "down_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=11008, out_features=4096, bias=False)"
                    },
                    "act_fn": {
                      "class": "SiLU",
                      "module": "SiLU()"
                    }
                  }
                },
                "input_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                },
                "post_attention_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                }
              }
            },
            "12": {
              "class": "LlamaDecoderLayer",
              "children": {
                "self_attn": {
                  "class": "LlamaAttention",
                  "children": {
                    "q_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "k_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "v_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "o_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    }
                  }
                },
                "mlp": {
                  "class": "LlamaMLP",
                  "children": {
                    "gate_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "up_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "down_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=11008, out_features=4096, bias=False)"
                    },
                    "act_fn": {
                      "class": "SiLU",
                      "module": "SiLU()"
                    }
                  }
                },
                "input_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                },
                "post_attention_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                }
              }
            },
            "13": {
              "class": "LlamaDecoderLayer",
              "children": {
                "self_attn": {
                  "class": "LlamaAttention",
                  "children": {
                    "q_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "k_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "v_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "o_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    }
                  }
                },
                "mlp": {
                  "class": "LlamaMLP",
                  "children": {
                    "gate_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "up_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "down_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=11008, out_features=4096, bias=False)"
                    },
                    "act_fn": {
                      "class": "SiLU",
                      "module": "SiLU()"
                    }
                  }
                },
                "input_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                },
                "post_attention_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                }
              }
            },
            "14": {
              "class": "LlamaDecoderLayer",
              "children": {
                "self_attn": {
                  "class": "LlamaAttention",
                  "children": {
                    "q_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "k_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "v_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "o_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    }
                  }
                },
                "mlp": {
                  "class": "LlamaMLP",
                  "children": {
                    "gate_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "up_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "down_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=11008, out_features=4096, bias=False)"
                    },
                    "act_fn": {
                      "class": "SiLU",
                      "module": "SiLU()"
                    }
                  }
                },
                "input_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                },
                "post_attention_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                }
              }
            },
            "15": {
              "class": "LlamaDecoderLayer",
              "children": {
                "self_attn": {
                  "class": "LlamaAttention",
                  "children": {
                    "q_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "k_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "v_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "o_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    }
                  }
                },
                "mlp": {
                  "class": "LlamaMLP",
                  "children": {
                    "gate_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "up_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "down_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=11008, out_features=4096, bias=False)"
                    },
                    "act_fn": {
                      "class": "SiLU",
                      "module": "SiLU()"
                    }
                  }
                },
                "input_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                },
                "post_attention_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                }
              }
            },
            "16": {
              "class": "LlamaDecoderLayer",
              "children": {
                "self_attn": {
                  "class": "LlamaAttention",
                  "children": {
                    "q_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "k_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "v_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "o_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    }
                  }
                },
                "mlp": {
                  "class": "LlamaMLP",
                  "children": {
                    "gate_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "up_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "down_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=11008, out_features=4096, bias=False)"
                    },
                    "act_fn": {
                      "class": "SiLU",
                      "module": "SiLU()"
                    }
                  }
                },
                "input_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                },
                "post_attention_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                }
              }
            },
            "17": {
              "class": "LlamaDecoderLayer",
              "children": {
                "self_attn": {
                  "class": "LlamaAttention",
                  "children": {
                    "q_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "k_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "v_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "o_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    }
                  }
                },
                "mlp": {
                  "class": "LlamaMLP",
                  "children": {
                    "gate_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "up_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "down_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=11008, out_features=4096, bias=False)"
                    },
                    "act_fn": {
                      "class": "SiLU",
                      "module": "SiLU()"
                    }
                  }
                },
                "input_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                },
                "post_attention_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                }
              }
            },
            "18": {
              "class": "LlamaDecoderLayer",
              "children": {
                "self_attn": {
                  "class": "LlamaAttention",
                  "children": {
                    "q_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "k_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "v_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "o_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    }
                  }
                },
                "mlp": {
                  "class": "LlamaMLP",
                  "children": {
                    "gate_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "up_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "down_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=11008, out_features=4096, bias=False)"
                    },
                    "act_fn": {
                      "class": "SiLU",
                      "module": "SiLU()"
                    }
                  }
                },
                "input_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                },
                "post_attention_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                }
              }
            },
            "19": {
              "class": "LlamaDecoderLayer",
              "children": {
                "self_attn": {
                  "class": "LlamaAttention",
                  "children": {
                    "q_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "k_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "v_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "o_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    }
                  }
                },
                "mlp": {
                  "class": "LlamaMLP",
                  "children": {
                    "gate_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "up_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "down_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=11008, out_features=4096, bias=False)"
                    },
                    "act_fn": {
                      "class": "SiLU",
                      "module": "SiLU()"
                    }
                  }
                },
                "input_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                },
                "post_attention_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                }
              }
            },
            "20": {
              "class": "LlamaDecoderLayer",
              "children": {
                "self_attn": {
                  "class": "LlamaAttention",
                  "children": {
                    "q_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "k_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "v_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "o_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    }
                  }
                },
                "mlp": {
                  "class": "LlamaMLP",
                  "children": {
                    "gate_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "up_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "down_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=11008, out_features=4096, bias=False)"
                    },
                    "act_fn": {
                      "class": "SiLU",
                      "module": "SiLU()"
                    }
                  }
                },
                "input_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                },
                "post_attention_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                }
              }
            },
            "21": {
              "class": "LlamaDecoderLayer",
              "children": {
                "self_attn": {
                  "class": "LlamaAttention",
                  "children": {
                    "q_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "k_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "v_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "o_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    }
                  }
                },
                "mlp": {
                  "class": "LlamaMLP",
                  "children": {
                    "gate_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "up_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "down_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=11008, out_features=4096, bias=False)"
                    },
                    "act_fn": {
                      "class": "SiLU",
                      "module": "SiLU()"
                    }
                  }
                },
                "input_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                },
                "post_attention_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                }
              }
            },
            "22": {
              "class": "LlamaDecoderLayer",
              "children": {
                "self_attn": {
                  "class": "LlamaAttention",
                  "children": {
                    "q_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "k_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "v_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "o_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    }
                  }
                },
                "mlp": {
                  "class": "LlamaMLP",
                  "children": {
                    "gate_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "up_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "down_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=11008, out_features=4096, bias=False)"
                    },
                    "act_fn": {
                      "class": "SiLU",
                      "module": "SiLU()"
                    }
                  }
                },
                "input_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                },
                "post_attention_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                }
              }
            },
            "23": {
              "class": "LlamaDecoderLayer",
              "children": {
                "self_attn": {
                  "class": "LlamaAttention",
                  "children": {
                    "q_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "k_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "v_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "o_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    }
                  }
                },
                "mlp": {
                  "class": "LlamaMLP",
                  "children": {
                    "gate_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "up_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "down_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=11008, out_features=4096, bias=False)"
                    },
                    "act_fn": {
                      "class": "SiLU",
                      "module": "SiLU()"
                    }
                  }
                },
                "input_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                },
                "post_attention_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                }
              }
            },
            "24": {
              "class": "LlamaDecoderLayer",
              "children": {
                "self_attn": {
                  "class": "LlamaAttention",
                  "children": {
                    "q_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "k_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "v_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "o_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    }
                  }
                },
                "mlp": {
                  "class": "LlamaMLP",
                  "children": {
                    "gate_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "up_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "down_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=11008, out_features=4096, bias=False)"
                    },
                    "act_fn": {
                      "class": "SiLU",
                      "module": "SiLU()"
                    }
                  }
                },
                "input_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                },
                "post_attention_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                }
              }
            },
            "25": {
              "class": "LlamaDecoderLayer",
              "children": {
                "self_attn": {
                  "class": "LlamaAttention",
                  "children": {
                    "q_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "k_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "v_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "o_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    }
                  }
                },
                "mlp": {
                  "class": "LlamaMLP",
                  "children": {
                    "gate_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "up_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "down_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=11008, out_features=4096, bias=False)"
                    },
                    "act_fn": {
                      "class": "SiLU",
                      "module": "SiLU()"
                    }
                  }
                },
                "input_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                },
                "post_attention_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                }
              }
            },
            "26": {
              "class": "LlamaDecoderLayer",
              "children": {
                "self_attn": {
                  "class": "LlamaAttention",
                  "children": {
                    "q_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "k_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "v_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "o_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    }
                  }
                },
                "mlp": {
                  "class": "LlamaMLP",
                  "children": {
                    "gate_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "up_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "down_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=11008, out_features=4096, bias=False)"
                    },
                    "act_fn": {
                      "class": "SiLU",
                      "module": "SiLU()"
                    }
                  }
                },
                "input_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                },
                "post_attention_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                }
              }
            },
            "27": {
              "class": "LlamaDecoderLayer",
              "children": {
                "self_attn": {
                  "class": "LlamaAttention",
                  "children": {
                    "q_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "k_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "v_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "o_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    }
                  }
                },
                "mlp": {
                  "class": "LlamaMLP",
                  "children": {
                    "gate_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "up_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "down_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=11008, out_features=4096, bias=False)"
                    },
                    "act_fn": {
                      "class": "SiLU",
                      "module": "SiLU()"
                    }
                  }
                },
                "input_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                },
                "post_attention_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                }
              }
            },
            "28": {
              "class": "LlamaDecoderLayer",
              "children": {
                "self_attn": {
                  "class": "LlamaAttention",
                  "children": {
                    "q_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "k_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "v_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "o_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    }
                  }
                },
                "mlp": {
                  "class": "LlamaMLP",
                  "children": {
                    "gate_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "up_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "down_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=11008, out_features=4096, bias=False)"
                    },
                    "act_fn": {
                      "class": "SiLU",
                      "module": "SiLU()"
                    }
                  }
                },
                "input_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                },
                "post_attention_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                }
              }
            },
            "29": {
              "class": "LlamaDecoderLayer",
              "children": {
                "self_attn": {
                  "class": "LlamaAttention",
                  "children": {
                    "q_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "k_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "v_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "o_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    }
                  }
                },
                "mlp": {
                  "class": "LlamaMLP",
                  "children": {
                    "gate_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "up_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "down_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=11008, out_features=4096, bias=False)"
                    },
                    "act_fn": {
                      "class": "SiLU",
                      "module": "SiLU()"
                    }
                  }
                },
                "input_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                },
                "post_attention_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                }
              }
            },
            "30": {
              "class": "LlamaDecoderLayer",
              "children": {
                "self_attn": {
                  "class": "LlamaAttention",
                  "children": {
                    "q_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "k_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "v_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "o_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    }
                  }
                },
                "mlp": {
                  "class": "LlamaMLP",
                  "children": {
                    "gate_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "up_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "down_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=11008, out_features=4096, bias=False)"
                    },
                    "act_fn": {
                      "class": "SiLU",
                      "module": "SiLU()"
                    }
                  }
                },
                "input_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                },
                "post_attention_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                }
              }
            },
            "31": {
              "class": "LlamaDecoderLayer",
              "children": {
                "self_attn": {
                  "class": "LlamaAttention",
                  "children": {
                    "q_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "k_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "v_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    },
                    "o_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=4096, bias=False)"
                    }
                  }
                },
                "mlp": {
                  "class": "LlamaMLP",
                  "children": {
                    "gate_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "up_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=4096, out_features=11008, bias=False)"
                    },
                    "down_proj": {
                      "class": "Linear",
                      "module": "Linear(in_features=11008, out_features=4096, bias=False)"
                    },
                    "act_fn": {
                      "class": "SiLU",
                      "module": "SiLU()"
                    }
                  }
                },
                "input_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                },
                "post_attention_layernorm": {
                  "class": "LlamaRMSNorm",
                  "module": "LlamaRMSNorm((4096,), eps=1e-05)"
                }
              }
            }
          }
        },
        "norm": {
          "class": "LlamaRMSNorm",
          "module": "LlamaRMSNorm((4096,), eps=1e-05)"
        },
        "rotary_emb": {
          "class": "LlamaRotaryEmbedding",
          "module": "LlamaRotaryEmbedding()"
        }
      }
    },
    "lm_head": {
      "class": "Linear",
      "module": "Linear(in_features=4096, out_features=32000, bias=False)"
    }
  }
}