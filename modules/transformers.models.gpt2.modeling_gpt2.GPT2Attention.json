{
  "class_name": "transformers.models.gpt2.modeling_gpt2.GPT2Attention",
  "kernels": [
    {
      "kernel_type": "composite",
      "operation": "QKV projection through Conv1D layer",
      "analysis": "Line 328: query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2). The c_attn module is a Conv1D layer (transformers.pytorch_utils.Conv1D) that projects from hidden_size to 3*hidden_size, which is then split into separate Q, K, V tensors. The Conv1D performs: output = input @ weight + bias. Input shape: (batch_size, seq_len, hidden_size). Output after projection: (batch_size, seq_len, 3*hidden_size). After split: 3 tensors of shape (batch_size, seq_len, hidden_size). The split is a view operation with zero cost. Parameter justification: batch_size and seq_len come from input tensor shape, hidden_size from config.hidden_size (line 152), and 3*hidden_size is the output dimension for combined QKV (line 174: Conv1D(3*self.embed_dim, self.embed_dim)).",
      "flops": "${transformers.pytorch_utils.Conv1D}(batch_size, seq_len, hidden_size, 3 * hidden_size)",
      "memory_access": {
        "read": "${transformers.pytorch_utils.Conv1D}(batch_size, seq_len, hidden_size, 3 * hidden_size)",
        "write": "${transformers.pytorch_utils.Conv1D}(batch_size, seq_len, hidden_size, 3 * hidden_size)"
      }
    },
    {
      "kernel_type": "basic",
      "operation": "Reshape and permute query tensor to split into attention heads",
      "analysis": "Line 330: query = self._split_heads(query, self.num_heads, self.head_dim). The _split_heads method (lines 290-296) reshapes from (batch_size, seq_len, hidden_size) to (batch_size, seq_len, num_heads, head_dim), then permutes to (batch_size, num_heads, seq_len, head_dim). The permute operation returns a view with different strides and requires memory access when the data needs to be made contiguous for subsequent operations. Total elements: batch_size * seq_len * num_heads * head_dim = batch_size * seq_len * hidden_size.",
      "flops": "0",
      "memory_access": {
        "read": "batch_size * seq_len * hidden_size * a_bytes",
        "write": "batch_size * seq_len * hidden_size * a_bytes"
      }
    },
    {
      "kernel_type": "basic",
      "operation": "Reshape and permute key tensor to split into attention heads",
      "analysis": "Line 331: key = self._split_heads(key, self.num_heads, self.head_dim). Same operation as line 330 but for the key tensor. Reshapes from (batch_size, seq_len, hidden_size) to (batch_size, num_heads, seq_len, head_dim). Total elements: batch_size * seq_len * hidden_size.",
      "flops": "0",
      "memory_access": {
        "read": "batch_size * seq_len * hidden_size * a_bytes",
        "write": "batch_size * seq_len * hidden_size * a_bytes"
      }
    },
    {
      "kernel_type": "basic",
      "operation": "Reshape and permute value tensor to split into attention heads",
      "analysis": "Line 332: value = self._split_heads(value, self.num_heads, self.head_dim). Same operation as lines 330-331 but for the value tensor. Reshapes from (batch_size, seq_len, hidden_size) to (batch_size, num_heads, seq_len, head_dim). Total elements: batch_size * seq_len * hidden_size.",
      "flops": "0",
      "memory_access": {
        "read": "batch_size * seq_len * hidden_size * a_bytes",
        "write": "batch_size * seq_len * hidden_size * a_bytes"
      }
    },
    {
      "kernel_type": "basic",
      "operation": "Concatenate cached keys with new keys (conditional: only when layer_past is not None)",
      "analysis": "Line 336: key = torch.cat((past_key, key), dim=-2). This operation executes only when layer_past is not None (cached inference scenario). Concatenates past_key of shape (batch_size, num_heads, cache_len, head_dim) with current key of shape (batch_size, num_heads, seq_len, head_dim) along the sequence dimension, resulting in (batch_size, num_heads, cache_len + seq_len, head_dim). torch.cat creates a new tensor by copying data from both inputs. Total elements in output: batch_size * num_heads * (cache_len + seq_len) * head_dim. This is a memory-only operation with no arithmetic FLOPs.",
      "flops": "0",
      "memory_access": {
        "read": "batch_size * num_heads * (cache_len + seq_len) * head_dim * a_bytes",
        "write": "batch_size * num_heads * (cache_len + seq_len) * head_dim * a_bytes"
      }
    },
    {
      "kernel_type": "basic",
      "operation": "Concatenate cached values with new values (conditional: only when layer_past is not None)",
      "analysis": "Line 337: value = torch.cat((past_value, value), dim=-2). Same as line 336 but for value tensors. Concatenates past_value of shape (batch_size, num_heads, cache_len, head_dim) with current value of shape (batch_size, num_heads, seq_len, head_dim), resulting in (batch_size, num_heads, cache_len + seq_len, head_dim). Total elements: batch_size * num_heads * (cache_len + seq_len) * head_dim. This is a memory-only operation.",
      "flops": "0",
      "memory_access": {
        "read": "batch_size * num_heads * (cache_len + seq_len) * head_dim * a_bytes",
        "write": "batch_size * num_heads * (cache_len + seq_len) * head_dim * a_bytes"
      }
    },
    {
      "kernel_type": "basic",
      "operation": "Multi-head attention computation via _attn method (Q*K^T, scale, mask, softmax, *V)",
      "analysis": "Line 347: attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask). The _attn method (lines 198-236) performs standard scaled dot-product attention. Operations: (1) Line 199: Q @ K^T matmul with query (batch_size, num_heads, seq_len, head_dim) and key transposed (batch_size, num_heads, head_dim, kv_len), producing attn_weights (batch_size, num_heads, seq_len, kv_len). FLOPs: 2*batch_size*num_heads*seq_len*kv_len*head_dim. (2) Line 201-204: Scale by 1/sqrt(head_dim), element-wise division. FLOPs: batch_size*num_heads*seq_len*kv_len. (3) Line 210-218: Apply causal mask using torch.where, element-wise selection. FLOPs: batch_size*num_heads*seq_len*kv_len. (4) Line 220-222: Add attention_mask if provided, element-wise addition. FLOPs: batch_size*num_heads*seq_len*kv_len. (5) Line 224: Softmax over last dimension (exp, sum, div). FLOPs: ~3*batch_size*num_heads*seq_len*kv_len. (6) Line 228: Dropout (no-op in inference mode). (7) Line 234: attn_weights @ V matmul with attn_weights (batch_size, num_heads, seq_len, kv_len) and value (batch_size, num_heads, kv_len, head_dim), producing attn_output (batch_size, num_heads, seq_len, head_dim). FLOPs: 2*batch_size*num_heads*seq_len*kv_len*head_dim. Total FLOPs: 4*batch_size*num_heads*seq_len*kv_len*head_dim + 6*batch_size*num_heads*seq_len*kv_len. Here kv_len = cache_len + seq_len (or just seq_len if no cache).",
      "flops": "4 * batch_size * num_heads * seq_len * (cache_len + seq_len) * head_dim + 6 * batch_size * num_heads * seq_len * (cache_len + seq_len)",
      "memory_access": {
        "read": "batch_size * num_heads * seq_len * head_dim * a_bytes + 2 * batch_size * num_heads * (cache_len + seq_len) * head_dim * a_bytes + batch_size * num_heads * seq_len * (cache_len + seq_len) * a_bytes",
        "write": "batch_size * num_heads * seq_len * (cache_len + seq_len) * a_bytes + batch_size * num_heads * seq_len * head_dim * a_bytes"
      }
    },
    {
      "kernel_type": "basic",
      "operation": "Merge attention heads back to hidden dimension",
      "analysis": "Line 349: attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim). The _merge_heads method (lines 298-304) permutes from (batch_size, num_heads, seq_len, head_dim) to (batch_size, seq_len, num_heads, head_dim) via permute(0,2,1,3), calls contiguous() to ensure contiguous memory layout (may trigger data copy), then reshapes to (batch_size, seq_len, hidden_size) where hidden_size = num_heads * head_dim. Total elements: batch_size * seq_len * hidden_size. This is a memory-only operation.",
      "flops": "0",
      "memory_access": {
        "read": "batch_size * seq_len * hidden_size * a_bytes",
        "write": "batch_size * seq_len * hidden_size * a_bytes"
      }
    },
    {
      "kernel_type": "composite",
      "operation": "Output projection through Conv1D layer",
      "analysis": "Line 350: attn_output = self.c_proj(attn_output). The c_proj module is a Conv1D layer (transformers.pytorch_utils.Conv1D) that projects from hidden_size to hidden_size. Input shape: (batch_size, seq_len, hidden_size). Output shape: (batch_size, seq_len, hidden_size). Parameter justification: batch_size and seq_len from propagated tensor shape, hidden_size from config.hidden_size (line 152). The Conv1D is defined at line 175: Conv1D(self.embed_dim, self.embed_dim), so both input and output dimensions are hidden_size.",
      "flops": "${transformers.pytorch_utils.Conv1D}(batch_size, seq_len, hidden_size, hidden_size)",
      "memory_access": {
        "read": "${transformers.pytorch_utils.Conv1D}(batch_size, seq_len, hidden_size, hidden_size)",
        "write": "${transformers.pytorch_utils.Conv1D}(batch_size, seq_len, hidden_size, hidden_size)"
      }
    },
    {
      "kernel_type": "composite",
      "operation": "Residual dropout (no-op in inference mode)",
      "analysis": "Line 351: attn_output = self.resid_dropout(attn_output). The resid_dropout is an nn.Dropout module (line 178: nn.Dropout(config.resid_pdrop)). During inference mode (model.eval()), dropout acts as an identity function and returns the input unchanged with no computation. Input/output shape: (batch_size, seq_len, hidden_size). Parameter justification: batch_size, seq_len, hidden_size from tensor shape. In inference mode, FLOPs = 0 and memory access = 0.",
      "flops": "${torch.nn.modules.dropout.Dropout}(batch_size, seq_len, hidden_size)",
      "memory_access": {
        "read": "${torch.nn.modules.dropout.Dropout}(batch_size, seq_len, hidden_size)",
        "write": "${torch.nn.modules.dropout.Dropout}(batch_size, seq_len, hidden_size)"
      }
    }
  ]
}
