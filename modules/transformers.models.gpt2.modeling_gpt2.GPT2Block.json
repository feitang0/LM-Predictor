{
  "class_name": "transformers.models.gpt2.modeling_gpt2.GPT2Block",
  "kernels": [
    {
      "kernel_type": "composite",
      "operation": "First LayerNorm - normalize hidden states before self-attention",
      "analysis": "Line 616: hidden_states = self.ln_1(hidden_states). The ln_1 module is torch.nn.LayerNorm (instantiated at line 594) that normalizes across the hidden_size dimension. Input shape: (batch_size, seq_len, hidden_size). Parameter justification: batch_size and seq_len come from input tensor dimensions; hidden_size is from config.hidden_size (line 590), used to instantiate ln_1 and determines the normalization dimension.",
      "flops": "${torch.nn.modules.normalization.LayerNorm}(batch_size, seq_len, hidden_size)",
      "memory_access": {
        "read": "${torch.nn.modules.normalization.LayerNorm}(batch_size, seq_len, hidden_size)",
        "write": "${torch.nn.modules.normalization.LayerNorm}(batch_size, seq_len, hidden_size)"
      }
    },
    {
      "kernel_type": "composite",
      "operation": "Self-attention computation with optional KV caching",
      "analysis": "Lines 617-624: attn_outputs = self.attn(hidden_states, layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask, use_cache=use_cache, output_attentions=output_attentions). The attn module is GPT2Attention (line 595, using attention_class from line 592). It performs multi-head self-attention over the normalized hidden states. Parameter justification: batch_size and seq_len from input shape; hidden_size from config.hidden_size (input/output dimension for Q,K,V projections); num_heads from config.n_head (splits attention into parallel heads); head_dim = hidden_size / num_heads (per-head dimension); cache_len from layer_past[0].shape[2] if provided, else 0 (determines cached sequence length for attention computation).",
      "flops": "${transformers.models.gpt2.modeling_gpt2.GPT2Attention}(batch_size, seq_len, hidden_size, num_heads, head_dim, cache_len)",
      "memory_access": {
        "read": "${transformers.models.gpt2.modeling_gpt2.GPT2Attention}(batch_size, seq_len, hidden_size, num_heads, head_dim, cache_len)",
        "write": "${transformers.models.gpt2.modeling_gpt2.GPT2Attention}(batch_size, seq_len, hidden_size, num_heads, head_dim, cache_len)"
      }
    },
    {
      "kernel_type": "basic",
      "operation": "First residual connection - element-wise addition of attention output with original input",
      "analysis": "Line 628: hidden_states = attn_output + residual. Element-wise addition of two tensors with shape (batch_size, seq_len, hidden_size). The attn_output is the result from self-attention, and residual is the original input stored at line 615. Each element requires one addition operation. Total elements: batch_size * seq_len * hidden_size. Memory access: read both tensors (2 reads) and write result (1 write).",
      "flops": "batch_size * seq_len * hidden_size",
      "memory_access": {
        "read": "2 * batch_size * seq_len * hidden_size * a_bytes",
        "write": "batch_size * seq_len * hidden_size * a_bytes"
      }
    },
    {
      "kernel_type": "composite",
      "operation": "Second LayerNorm - normalize hidden states before MLP",
      "analysis": "Line 653: hidden_states = self.ln_2(hidden_states). The ln_2 module is torch.nn.LayerNorm (instantiated at line 596) that normalizes across the hidden_size dimension. Input shape: (batch_size, seq_len, hidden_size). Parameter justification: batch_size and seq_len from input tensor dimensions; hidden_size from config.hidden_size (line 590), used to instantiate ln_2 and determines the normalization dimension.",
      "flops": "${torch.nn.modules.normalization.LayerNorm}(batch_size, seq_len, hidden_size)",
      "memory_access": {
        "read": "${torch.nn.modules.normalization.LayerNorm}(batch_size, seq_len, hidden_size)",
        "write": "${torch.nn.modules.normalization.LayerNorm}(batch_size, seq_len, hidden_size)"
      }
    },
    {
      "kernel_type": "composite",
      "operation": "MLP (feed-forward network) transformation",
      "analysis": "Line 654: feed_forward_hidden_states = self.mlp(hidden_states). The mlp module is GPT2MLP (instantiated at line 602 with inner_dim parameter). GPT2MLP consists of two Conv1D layers (expansion and contraction) with activation and dropout. Input/output shape: (batch_size, seq_len, hidden_size). Parameter justification: batch_size and seq_len from input tensor dimensions; hidden_size from config.hidden_size (line 590, the embed_dim in GPT2MLP line 567), used for input/output dimensions; intermediate_size from config.n_inner or defaults to 4 * hidden_size (line 591), used for the expansion layer (line 568: c_fc = Conv1D(intermediate_size, embed_dim)).",
      "flops": "${transformers.models.gpt2.modeling_gpt2.GPT2MLP}(batch_size, seq_len, hidden_size, intermediate_size)",
      "memory_access": {
        "read": "${transformers.models.gpt2.modeling_gpt2.GPT2MLP}(batch_size, seq_len, hidden_size, intermediate_size)",
        "write": "${transformers.models.gpt2.modeling_gpt2.GPT2MLP}(batch_size, seq_len, hidden_size, intermediate_size)"
      }
    },
    {
      "kernel_type": "basic",
      "operation": "Second residual connection - element-wise addition of MLP output with input to MLP",
      "analysis": "Line 656: hidden_states = residual + feed_forward_hidden_states. Element-wise addition of two tensors with shape (batch_size, seq_len, hidden_size). The residual is stored at line 652 (input to MLP block), and feed_forward_hidden_states is the MLP output. Each element requires one addition operation. Total elements: batch_size * seq_len * hidden_size. Memory access: read both tensors (2 reads) and write result (1 write).",
      "flops": "batch_size * seq_len * hidden_size",
      "memory_access": {
        "read": "2 * batch_size * seq_len * hidden_size * a_bytes",
        "write": "batch_size * seq_len * hidden_size * a_bytes"
      }
    }
  ]
}
