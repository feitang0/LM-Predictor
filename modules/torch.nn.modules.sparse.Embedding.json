{
  "class_name": "torch.nn.modules.sparse.Embedding",
  "kernels": [
    {
      "kernel_type": "basic",
      "operation": "Embedding lookup via torch.embedding",
      "analysis": "The forward method (lines 192-200) calls F.embedding() which delegates to torch.embedding() (line 2546 of functional.py). This performs an embedding table lookup operation: for each index in the input tensor, it retrieves the corresponding embedding vector from the weight matrix. The operation is purely a memory gather/index operation with no arithmetic computation. Input shape is typically (batch_size, seq_len) containing indices, weight matrix has shape (num_embeddings, embedding_dim), and output has shape (batch_size, seq_len, embedding_dim). For general n-dimensional input of shape (*), output is (*, embedding_dim). The total number of indices is num_indices = product of all input dimensions (e.g., batch_size * seq_len for 2D input). Memory access includes reading num_indices embedding vectors from the weight matrix (num_indices * embedding_dim * w_bytes), reading the indices themselves (num_indices * 8 bytes for LongTensor), and writing the gathered embeddings to output (num_indices * embedding_dim * a_bytes).",
      "flops": "0",
      "memory_access": {
        "read": "num_indices * embedding_dim * w_bytes + num_indices * 8",
        "write": "num_indices * embedding_dim * a_bytes"
      }
    }
  ]
}
